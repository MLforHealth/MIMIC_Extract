{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import pickle \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../utils')\n",
    "from simple_impute import simple_imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVENTION = 'vent'\n",
    "RANDOM = 0\n",
    "MAX_LEN = 240\n",
    "SLICE_SIZE = 6\n",
    "GAP_TIME = 6\n",
    "PREDICTION_WINDOW = 4\n",
    "OUTCOME_TYPE = 'all'\n",
    "NUM_CLASSES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_KEY = {'ONSET': 0, 'CONTROL': 1, 'ON_INTERVENTION': 2, 'WEAN': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = '../data/curated/all_hourly_data.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_hdf(DATAFILE,'vitals_labs')\n",
    "Y = pd.read_hdf(DATAFILE,'interventions')\n",
    "static = pd.read_hdf(DATAFILE,'patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y[[INTERVENTION]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X :  (2200954, 123)\n",
      "Shape of Y :  (2200954, 1)\n",
      "Shape of static :  (34472, 17)\n"
     ]
    }
   ],
   "source": [
    "print ('Shape of X : ', X.shape)\n",
    "print ('Shape of Y : ', Y.shape)\n",
    "print ('Shape of static : ', static.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split, Stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, test_ids = train_test_split(static.reset_index(), test_size=0.2, \n",
    "                                       random_state=RANDOM, stratify=static['mort_hosp'])\n",
    "split_train_ids, val_ids = train_test_split(train_ids, test_size=0.125, \n",
    "                                            random_state=RANDOM, stratify=train_ids['mort_hosp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation and Standardization of Time Series Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "X_clean = simple_imputer(X,train_ids['subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(x):# normalize\n",
    "    mins = x.min()\n",
    "    maxes = x.max()\n",
    "    x_std = (x - mins) / (maxes - mins)\n",
    "    return x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_time_since_measurement(x):\n",
    "    idx = pd.IndexSlice\n",
    "    x = np.where(x==100, 0, x)\n",
    "    means = x.mean()\n",
    "    stds = x.std()\n",
    "    x_std = (x - means)/stds\n",
    "    return x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "X_std = X_clean.copy()\n",
    "X_std.loc[:,idx[:,'mean']] = X_std.loc[:,idx[:,'mean']].apply(lambda x: minmax(x))\n",
    "X_std.loc[:,idx[:,'time_since_measured']] = X_std.loc[:,idx[:,'time_since_measured']].apply(lambda x: std_time_since_measurement(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std.columns = X_std.columns.droplevel(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorization of Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_age(age):\n",
    "    if age > 10 and age <= 30: \n",
    "        cat = 1\n",
    "    elif age > 30 and age <= 50:\n",
    "        cat = 2\n",
    "    elif age > 50 and age <= 70:\n",
    "        cat = 3\n",
    "    else: \n",
    "        cat = 4\n",
    "    return cat\n",
    "\n",
    "def categorize_ethnicity(ethnicity):\n",
    "    if 'AMERICAN INDIAN' in ethnicity:\n",
    "        ethnicity = 'AMERICAN INDIAN'\n",
    "    elif 'ASIAN' in ethnicity:\n",
    "        ethnicity = 'ASIAN'\n",
    "    elif 'WHITE' in ethnicity:\n",
    "        ethnicity = 'WHITE'\n",
    "    elif 'HISPANIC' in ethnicity:\n",
    "        ethnicity = 'HISPANIC/LATINO'\n",
    "    elif 'BLACK' in ethnicity:\n",
    "        ethnicity = 'BLACK'\n",
    "    else: \n",
    "        ethnicity = 'OTHER'\n",
    "    return ethnicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gender, first_careunit, age and ethnicity for prediction\n",
    "static_to_keep = static[['gender', 'age', 'ethnicity', 'first_careunit', 'intime']]\n",
    "static_to_keep.loc[:, 'intime'] = static_to_keep['intime'].astype('datetime64').apply(lambda x : x.hour)\n",
    "static_to_keep.loc[:, 'age'] = static_to_keep['age'].apply(categorize_age)\n",
    "static_to_keep.loc[:, 'ethnicity'] = static_to_keep['ethnicity'].apply(categorize_ethnicity)\n",
    "static_to_keep = pd.get_dummies(static_to_keep, columns = ['gender', 'age', 'ethnicity', 'first_careunit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge time series and static data\n",
    "X_merge = pd.merge(X_std.reset_index(), static_to_keep.reset_index(), on=['subject_id','icustay_id','hadm_id'])\n",
    "# add absolute time feature\n",
    "abs_time = (X_merge['intime'] + X_merge['hours_in'])%24\n",
    "X_merge.insert(4, 'absolute_time', abs_time)\n",
    "X_merge.drop('intime', axis=1, inplace=True)\n",
    "X_merge = X_merge.set_index(['subject_id','icustay_id','hadm_id','hours_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_std, X_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_x_matrix(x):\n",
    "    zeros = np.zeros((MAX_LEN, x.shape[1]-4))\n",
    "    x = x.values\n",
    "    x = x[:(MAX_LEN), 4:]\n",
    "    zeros[0:x.shape[0], :] = x\n",
    "    return zeros\n",
    "\n",
    "def create_y_matrix(y):\n",
    "    zeros = np.zeros((MAX_LEN, y.shape[1]-4))\n",
    "    y = y.values\n",
    "    y = y[:,4:]\n",
    "    y = y[:MAX_LEN, :]\n",
    "    zeros[:y.shape[0], :] = y\n",
    "    return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(X_merge.reset_index().groupby('subject_id').apply(create_x_matrix)))\n",
    "y = np.array(list(Y.reset_index().groupby('subject_id').apply(create_y_matrix)))[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lengths = np.array(list(X_merge.reset_index().groupby('subject_id').apply(lambda x: x.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = pd.Series(X_merge.reset_index()['subject_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor shape:  (34472, 240, 141)\n",
      "Y tensor shape:  (34472, 240)\n",
      "lengths shape:  (34472,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X tensor shape: \", x.shape)\n",
    "print(\"Y tensor shape: \", y.shape)\n",
    "print(\"lengths shape: \", lengths.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.where(keys.isin(train_ids['subject_id']))[0]\n",
    "test_indices = np.where(keys.isin(test_ids['subject_id']))[0]\n",
    "train_static = train_ids\n",
    "split_train_indices = np.where(keys.isin(split_train_ids['subject_id']))[0]\n",
    "val_indices = np.where(keys.isin(val_ids['subject_id']))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x[split_train_indices]\n",
    "Y_train = y[split_train_indices]\n",
    "X_test = x[test_indices]\n",
    "Y_test = y[test_indices]\n",
    "X_val = x[val_indices]\n",
    "Y_val = y[val_indices]\n",
    "lengths_train = lengths[split_train_indices]\n",
    "lengths_val = lengths[val_indices]\n",
    "lengths_test = lengths[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:  24129\n",
      "Validation size:  3448\n",
      "Test size:  6895\n"
     ]
    }
   ],
   "source": [
    "print(\"Training size: \", X_train.shape[0])\n",
    "print(\"Validation size: \", X_val.shape[0])\n",
    "print(\"Test size: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_3d_tensor_slices(X_tensor, Y_tensor, lengths):\n",
    "\n",
    "    num_patients = X_tensor.shape[0]\n",
    "    timesteps = X_tensor.shape[1]\n",
    "    num_features = X_tensor.shape[2]\n",
    "    X_tensor_new = np.zeros((lengths.sum(), SLICE_SIZE, num_features + 1))\n",
    "    Y_tensor_new = np.zeros((lengths.sum()))\n",
    "\n",
    "    current_row = 0\n",
    "    \n",
    "    for patient_index in range(num_patients):\n",
    "        x_patient = X_tensor[patient_index]\n",
    "        y_patient = Y_tensor[patient_index]\n",
    "        length = lengths[patient_index]\n",
    "\n",
    "        for timestep in range(length - PREDICTION_WINDOW - GAP_TIME - SLICE_SIZE):\n",
    "            x_window = x_patient[timestep:timestep+SLICE_SIZE]\n",
    "            y_window = y_patient[timestep:timestep+SLICE_SIZE]\n",
    "            x_window = np.concatenate((x_window, np.expand_dims(y_window,1)), axis=1)\n",
    "            result_window = y_patient[timestep+SLICE_SIZE+GAP_TIME:timestep+SLICE_SIZE+GAP_TIME+PREDICTION_WINDOW]\n",
    "            result_window_diff = set(np.diff(result_window))\n",
    "            #if 1 in result_window_diff: pdb.set_trace()\n",
    "            gap_window = y_patient[timestep+SLICE_SIZE:timestep+SLICE_SIZE+GAP_TIME]\n",
    "            gap_window_diff = set(np.diff(gap_window))\n",
    "\n",
    "            #print result_window, result_window_diff\n",
    "\n",
    "            if OUTCOME_TYPE == 'binary':\n",
    "                if max(gap_window) == 1:\n",
    "                    result = None\n",
    "                elif max(result_window) == 1:\n",
    "                    result = 1\n",
    "                elif max(result_window) == 0:\n",
    "                    result = 0\n",
    "                if result != None:\n",
    "                    X_tensor_new[current_row] = x_window\n",
    "                    Y_tensor_new[current_row] = result\n",
    "                    current_row += 1\n",
    "\n",
    "            else: \n",
    "                if 1 in gap_window_diff or -1 in gap_window_diff:\n",
    "                    result = None\n",
    "                elif (len(result_window_diff) == 1) and (0 in result_window_diff) and (max(result_window) == 0):\n",
    "                    result = CHUNK_KEY['CONTROL']\n",
    "                elif (len(result_window_diff) == 1) and (0 in result_window_diff) and (max(result_window) == 1):\n",
    "                    result = CHUNK_KEY['ON_INTERVENTION']\n",
    "                elif 1 in result_window_diff: \n",
    "                    result = CHUNK_KEY['ONSET']\n",
    "                elif -1 in result_window_diff:\n",
    "                    result = CHUNK_KEY['WEAN']\n",
    "                else:\n",
    "                    result = None\n",
    "\n",
    "                if result != None:\n",
    "                    X_tensor_new[current_row] = x_window\n",
    "                    Y_tensor_new[current_row] = result\n",
    "                    current_row += 1\n",
    "\n",
    "    X_tensor_new = X_tensor_new[:current_row,:,:]\n",
    "    Y_tensor_new = Y_tensor_new[:current_row]\n",
    "\n",
    "    return X_tensor_new, Y_tensor_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = make_3d_tensor_slices(X_train, Y_train, lengths_train)\n",
    "x_val, y_val = make_3d_tensor_slices(X_val, Y_val, lengths_val)\n",
    "x_test, y_test = make_3d_tensor_slices(X_test, Y_test, lengths_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_classes = label_binarize(y_train, classes=range(NUM_CLASSES))\n",
    "y_val_classes = label_binarize(y_val, classes=range(NUM_CLASSES))\n",
    "y_test_classes = label_binarize(y_test, classes=range(NUM_CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, Y_train, X_test, Y_test, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train:  (1107493, 6, 142)\n",
      "shape of x_val:  (161025, 6, 142)\n",
      "shape of x_test:  (314548, 6, 142)\n"
     ]
    }
   ],
   "source": [
    "print('shape of x_train: ', x_train.shape)\n",
    "print('shape of x_val: ', x_val.shape)\n",
    "print('shape of x_test: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_col = 17 #static_to_keep.shape[1] - 1\n",
    "time_series_col = 124 #X_merge.shape[1] - static_col _shape_repr(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_static(x):\n",
    "    x_static = x[:,0,time_series_col:x.shape[2]-1]\n",
    "    x_timeseries = np.reshape(x[:,:,:time_series_col],(x.shape[0], -1))\n",
    "    x_int = x[:,:,-1]\n",
    "    x_concat = np.concatenate((x_static, x_timeseries, x_int), axis=1)\n",
    "    return x_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate hourly features\n",
    "x_train_concat = remove_duplicate_static(x_train)\n",
    "x_val_concat = remove_duplicate_static(x_val)\n",
    "x_test_concat = remove_duplicate_static(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1107493, 767)\n",
      "(161025, 767)\n",
      "(314548, 767)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_concat.shape)\n",
    "print(x_val_concat.shape)\n",
    "print(x_test_concat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictDist():\n",
    "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
    "    def rvs(self, n):\n",
    "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
    "        out = []\n",
    "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
    "        return out\n",
    "    \n",
    "class Choice():\n",
    "    def __init__(self, options): self.options = options\n",
    "    def rvs(self, n): return [self.options[i] for i in ss.randint(0, len(self.options)).rvs(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "np.random.seed(RANDOM)\n",
    "LR_dist = DictDist({\n",
    "    'C': Choice(np.geomspace(1e-3, 1e3, 10000)),\n",
    "    'penalty': Choice(['l2']),\n",
    "    'solver': Choice(['sag']),\n",
    "    'max_iter': Choice([100, 200]),\n",
    "    'class_weight': Choice(['balanced']),\n",
    "    'multi_class': Choice(['multinomial']),\n",
    "    'random_state': Choice([RANDOM])\n",
    "})\n",
    "LR_hyperparams_list = LR_dist.rvs(N)\n",
    "        \n",
    "RF_dist = DictDist({\n",
    "    'n_estimators': ss.randint(50, 200),\n",
    "    'max_depth': ss.randint(2, 10),\n",
    "    'min_samples_split': ss.randint(2, 75),\n",
    "    'min_samples_leaf': ss.randint(1, 50),\n",
    "    'class_weight': Choice(['balanced']),\n",
    "    'random_state': Choice([RANDOM])\n",
    "})\n",
    "RF_hyperparams_list = RF_dist.rvs(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_basic(model, hyperparams_list, X_train, X_val, X_test):\n",
    "    best_s, best_hyperparams = -np.Inf, None\n",
    "    for i, hyperparams in enumerate(hyperparams_list):\n",
    "        M = model(**hyperparams)\n",
    "        M.fit(X_train, y_train)\n",
    "        s = roc_auc_score(y_val_classes, M.predict_proba(X_val),average='macro')\n",
    "        if s > best_s:\n",
    "            best_s, best_hyperparams = s, hyperparams\n",
    "\n",
    "    return run_only_final(model, best_hyperparams, X_train, X_val, X_test)\n",
    "\n",
    "def run_only_final(model, best_hyperparams, X_train, X_val, X_test):\n",
    "    best_M = model(**best_hyperparams)\n",
    "    best_M.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "    y_pred  = best_M.predict_proba(X_test)\n",
    "    auc   = roc_auc_score(y_test_classes, y_pred, average=None)\n",
    "    aucmacro = roc_auc_score(y_test_classes, y_pred, average='macro')\n",
    "    \n",
    "    return auc, aucmacro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results for model RF, (AUC, Macro_AUC)\n",
      "(array([0.8707733 , 0.98981498, 0.9852656 , 0.93999635]), 0.9464625569824558)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results for model LR, (AUC, Macro_AUC)\n",
      "(array([0.71889636, 0.98294004, 0.98429292, 0.93236335]), 0.9046231696007481)\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for model_name, model, hyperparams_list in [('RF', RandomForestClassifier, RF_hyperparams_list), \n",
    "                                            ('LR', LogisticRegression, LR_hyperparams_list)]:\n",
    "    if model_name not in results: results[model_name] = {}\n",
    "\n",
    "    results[model_name] = run_basic(\n",
    "        model, hyperparams_list, x_train_concat, x_val_concat, x_test_concat)\n",
    "    print(\"Final results for model %s, (AUC, Macro_AUC)\" % (model_name))\n",
    "    print(results[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, RepeatVector, Lambda\n",
    "from keras.layers import Input, Conv2D, Conv1D, Conv3D, MaxPooling2D, MaxPooling1D\n",
    "from keras.layers import Concatenate\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import random as rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 12\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "#class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weight = [1,1,1,1]\n",
    "class_weight = dict(zip(range(len(class_weight)), class_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shirlyw/anaconda3/envs/mimic_extract_py36/lib/python3.6/site-packages/ipykernel_launcher.py:45: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1107493 samples, validate on 161025 samples\n",
      "Epoch 1/12\n",
      "1107493/1107493 [==============================] - 86s 78us/step - loss: 0.1548 - acc: 0.9612 - val_loss: 0.1369 - val_acc: 0.9644\n",
      "Epoch 2/12\n",
      "1107493/1107493 [==============================] - 84s 76us/step - loss: 0.1384 - acc: 0.9645 - val_loss: 0.1363 - val_acc: 0.9647\n",
      "Epoch 3/12\n",
      "1107493/1107493 [==============================] - 83s 75us/step - loss: 0.1355 - acc: 0.9649 - val_loss: 0.1345 - val_acc: 0.9649\n",
      "Epoch 4/12\n",
      "1107493/1107493 [==============================] - 83s 75us/step - loss: 0.1333 - acc: 0.9651 - val_loss: 0.1393 - val_acc: 0.9643\n",
      "Epoch 5/12\n",
      "1107493/1107493 [==============================] - 83s 75us/step - loss: 0.1316 - acc: 0.9652 - val_loss: 0.1433 - val_acc: 0.9640\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc3206f3cc0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "\n",
    "np.random.seed(RANDOM)\n",
    "set_random_seed(RANDOM)\n",
    "rn.seed(RANDOM)\n",
    "\n",
    "input_shape = (x_train.shape[1], x_train.shape[2])\n",
    "inputs = Input(shape=input_shape)\n",
    "model = Conv1D(64, kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,\n",
    "                 padding='same',\n",
    "                 name='conv2')(inputs)\n",
    "\n",
    "model = (MaxPooling1D(pool_size=3, strides=1))(model)\n",
    "\n",
    "model2 = Conv1D(64, kernel_size=4,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,\n",
    "                 padding='same',\n",
    "                 name='conv3')(inputs)\n",
    "\n",
    "model2 = MaxPooling1D(pool_size=3, strides=1)(model2)\n",
    "\n",
    "model3 = Conv1D(64, kernel_size=5,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,\n",
    "                 padding='same',\n",
    "                 name='conv4')(inputs)\n",
    "\n",
    "model3 = MaxPooling1D(pool_size=3, strides=1)(model3)\n",
    "\n",
    "models = [model, model2, model3]\n",
    "\n",
    "full_model = keras.layers.concatenate(models)\n",
    "full_model = Flatten()(full_model)\n",
    "full_model = Dense(128, activation='relu')(full_model)\n",
    "full_model = Dropout(DROPOUT)(full_model)\n",
    "full_model = Dense(NUM_CLASSES, activation='softmax')(full_model)\n",
    "\n",
    "full_model = keras.models.Model(input=inputs, outputs=full_model)\n",
    "\n",
    "full_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=.0005),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "full_model.fit(x_train, y_train_classes,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=1,\n",
    "          class_weight=class_weight,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_val, y_val_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72248416 0.98160299 0.98529343 0.9368696 ]\n",
      "0.9065625455406378\n"
     ]
    }
   ],
   "source": [
    "test_preds_cnn = full_model.predict(x_test, batch_size=BATCH_SIZE)\n",
    "print(roc_auc_score(y_test_classes, test_preds_cnn, average=None))\n",
    "print(roc_auc_score(y_test_classes, test_preds_cnn, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from sklearn.utils import class_weight\n",
    "#class_weight = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "class_weight = [1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 12\n",
    "KEEP_PROB = 0.8\n",
    "REGULARIZATION = 0.001\n",
    "NUM_HIDDEN = [512, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lazy_property(function):\n",
    "    attribute = '_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def wrapper(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class VariableSequenceLabelling:\n",
    "\n",
    "    def __init__(self, data, target, dropout_prob, reg, num_hidden, class_weights):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.reg = reg\n",
    "        self._num_hidden = num_hidden\n",
    "        self._num_layers = len(num_hidden)\n",
    "        self.num_classes = len(class_weights)\n",
    "        self.attn_length = 0\n",
    "        self.class_weights = class_weights\n",
    "        self.prediction\n",
    "        self.error\n",
    "        self.optimize\n",
    "\n",
    "    @lazy_property\n",
    "    def make_rnn_cell(self,\n",
    "                      attn_length=0,\n",
    "                      base_cell=tf.nn.rnn_cell.BasicLSTMCell,\n",
    "                      state_is_tuple=True):\n",
    "\n",
    "        attn_length = self.attn_length\n",
    "        input_dropout = self.dropout_prob\n",
    "        output_dropout = self.dropout_prob\n",
    "\n",
    "        cells = []\n",
    "        for num_units in self._num_hidden:\n",
    "            cell = base_cell(num_units, state_is_tuple=state_is_tuple)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=input_dropout, output_keep_prob=output_dropout,\n",
    "                                                seed=RANDOM)\n",
    "            cells.append(cell)\n",
    "\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=state_is_tuple)\n",
    "\n",
    "        return cell\n",
    "\n",
    "\n",
    "    # predictor for slices\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "\n",
    "        cell = self.make_rnn_cell\n",
    "\n",
    "        # Recurrent network.\n",
    "        output, final_state = tf.nn.dynamic_rnn(cell,\n",
    "            self.data,\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "\n",
    "        with tf.variable_scope(\"model\") as scope:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "            # final weights\n",
    "            num_classes = self.num_classes\n",
    "            weight, bias = self._weight_and_bias(self._num_hidden[-1], num_classes)\n",
    "    \n",
    "            # flatten + sigmoid\n",
    "            if self.attn_length > 0: \n",
    "                logits = tf.matmul(final_state[0][-1][-1], weight) + bias\n",
    "            else: \n",
    "                logits = tf.matmul(final_state[-1][-1], weight) + bias\n",
    "\n",
    "            prediction = tf.nn.softmax(logits)\n",
    "            \n",
    "            return logits, prediction\n",
    "\n",
    "        \n",
    "    @lazy_property\n",
    "    def cross_ent(self):\n",
    "        predictions = self.prediction[0]\n",
    "        real = tf.cast(tf.squeeze(self.target), tf.int32)\n",
    "\n",
    "        class_weight = tf.expand_dims(tf.cast(self.class_weights, tf.int32), axis=0)\n",
    "        one_hot_labels = tf.cast(tf.one_hot(real, depth=self.num_classes), tf.int32)\n",
    "        weight_per_label = tf.cast(tf.transpose(tf.matmul(one_hot_labels, tf.transpose(class_weight))), tf.float32) #shape [1, batch_size]\n",
    "\n",
    "        xent = tf.multiply(weight_per_label, tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=predictions, name=\"xent_raw\")) #shape [1, batch_size]\n",
    "        loss = tf.reduce_mean(xent) #shape 1\n",
    "        ce = loss\n",
    "        l2 = self.reg * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "        ce += l2\n",
    "        return ce\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        learning_rate = 0.0003\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        return optimizer.minimize(self.cross_ent)\n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        prediction = tf.argmax(self.prediction[1], 1)\n",
    "        real = tf.cast(self.target, tf.int32)\n",
    "        prediction = tf.cast(prediction, tf.int32)\n",
    "        mistakes = tf.not_equal(real, prediction)\n",
    "        mistakes = tf.cast(mistakes, tf.float32)\n",
    "        mistakes = tf.reduce_sum(mistakes, reduction_indices=0)\n",
    "        total = 128\n",
    "        mistakes = tf.divide(mistakes, tf.to_float(total))\n",
    "        return mistakes\n",
    "\n",
    "    @staticmethod\n",
    "    def _weight_and_bias(in_size, out_size):\n",
    "        weight = tf.truncated_normal([in_size, out_size], stddev=0.01)\n",
    "        bias = tf.constant(0.1, shape=[out_size])\n",
    "        return tf.Variable(weight), tf.Variable(bias)\n",
    "\n",
    "\n",
    "    @lazy_property\n",
    "    def summaries(self):\n",
    "        tf.summary.scalar('loss', tf.reduce_mean(self.cross_ent))\n",
    "        tf.summary.scalar('error', self.error)\n",
    "        merged = tf.summary.merge_all()\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test AUC:  [0.64618856 0.98296493 0.98584314 0.93222853] at epoch  3\n",
      "Best Test AUC Macro:  0.8868062912039549 at epoch  3\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "set_random_seed(RANDOM)\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "# if attn_length > 0:\n",
    "#     # weights file initialized\n",
    "#     weight_file = 'weights.txt'\n",
    "#     with open(weight_file, 'a') as the_file:\n",
    "#         pass\n",
    "\n",
    "with tf.Session(config = config) as sess, tf.device('/gpu:0'):\n",
    "    _, length, num_features = x_train.shape\n",
    "    num_data_cols = num_features\n",
    "\n",
    "    # placeholders\n",
    "    data = tf.placeholder(tf.float32, [None, length, num_data_cols])\n",
    "    target = tf.placeholder(tf.float32, [None])\n",
    "    dropout_prob = tf.placeholder(tf.float32)\n",
    "    reg = tf.placeholder(tf.float32)\n",
    "\n",
    "    # initialization\n",
    "    model = VariableSequenceLabelling(data, target, dropout_prob, reg, num_hidden=NUM_HIDDEN, class_weights=class_weight)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    \n",
    "    batch_size = BATCH_SIZE\n",
    "    dp = KEEP_PROB\n",
    "    rp = REGULARIZATION\n",
    "    train_samples = x_train.shape[0]\n",
    "    indices = list(range(train_samples))\n",
    "    num_classes = NUM_CLASSES\n",
    "    \n",
    "    # for storing results\n",
    "    test_data = x_test\n",
    "    val_data = x_val\n",
    "\n",
    "    val_aucs = []\n",
    "    test_aucs = []\n",
    "    val_aucs_macro = []\n",
    "    test_aucs_macro = []\n",
    "    \n",
    "    epoch = -1\n",
    "\n",
    "    while (epoch < 3 or max(np.diff(early_stop[-3:])) > 0):\n",
    "        epoch += 1\n",
    "        np.random.seed(RANDOM)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        num_batches = train_samples//batch_size\n",
    "        for batch_index in range(num_batches):\n",
    "\n",
    "            sample_indices = indices[batch_index*batch_size:batch_index*batch_size+batch_size]\n",
    "            batch_data = x_train[sample_indices, :, :num_data_cols]\n",
    "            batch_target = y_train[sample_indices]\n",
    "            _, loss = sess.run([model.optimize, model.cross_ent], {data: batch_data, target: batch_target, dropout_prob: dp, reg: rp})\n",
    "\n",
    "            # write train accuracy to log files every 10 batches\n",
    "            #if batch_index % 2000 == 0:\n",
    "            #    loss, prediction, error = sess.run([model.cross_ent, model.prediction, model.error], {data: batch_data, target: batch_target, dropout_prob: dp, reg: rp})\n",
    "            #    #train_writer.add_summary(summaries, global_step=epoch*batch_index)\n",
    "            #    print('Epoch {:2d} Batch {:2d}'.format(epoch+1, batch_index))\n",
    "            #    print('Loss = ', np.mean(loss))\n",
    "            #    print('Error = ', error)\n",
    "\n",
    "        cur_val_preds = sess.run(model.prediction, {data: x_val, target: y_val, dropout_prob: 1, reg: rp}) \n",
    "        val_preds = cur_val_preds[1]\n",
    "        \n",
    "        cur_test_preds = sess.run(model.prediction, {data: x_test, target: y_test, dropout_prob: 1, reg: rp}) \n",
    "        test_preds = cur_test_preds[1]\n",
    "        \n",
    "        val_auc_macro = roc_auc_score(y_val_classes, val_preds, average='macro')\n",
    "        test_auc_macro = roc_auc_score(y_test_classes, test_preds, average='macro')\n",
    "        val_aucs_macro.append(val_auc_macro)\n",
    "        test_aucs_macro.append(test_auc_macro)\n",
    "\n",
    "        val_auc = roc_auc_score(y_val_classes, val_preds, average=None)\n",
    "        test_auc = roc_auc_score(y_test_classes, test_preds, average=None)\n",
    "        val_aucs.append(val_auc)\n",
    "        test_aucs.append(test_auc)\n",
    "        \n",
    "        if isinstance(val_aucs_macro[-1], dict):\n",
    "            early_stop = [val_auc_macro for val_auc_macro in val_aucs_macro]\n",
    "        else: \n",
    "            early_stop = val_aucs_macro\n",
    "\n",
    "\n",
    "    if isinstance(val_aucs_macro[-1], dict):\n",
    "        best_epoch = np.argmax(np.array([val_auc_macro for val_auc_macro in val_aucs_macro]))\n",
    "    else: \n",
    "        best_epoch = np.argmax(val_aucs_macro)\n",
    "\n",
    "    best_val_auc = val_aucs[best_epoch]\n",
    "    best_test_auc = test_aucs[best_epoch]\n",
    "    best_test_auc_macro = test_aucs_macro[best_epoch]\n",
    "\n",
    "    print('Best Test AUC: ', best_test_auc, 'at epoch ', best_epoch)\n",
    "    print('Best Test AUC Macro: ', best_test_auc_macro, 'at epoch ', best_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
